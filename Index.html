
<html>
    <head>
        <link href="https://fonts.googleapis.com/css?family=PT+Sans" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Poiret+One" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Oswald|Poiret+One|Teko:600" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Roboto+Condensed" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css?family=Lato:900" rel="stylesheet">
        <style>
            body, html {
                height: 100%;
                background-color: #eeeeee;
            }
    
            .parallax {
                /* The image used */
                background-image: url('title.png');
                object-fit: cover;
    
                /* Full height */
                height: 40%;
    
                /* Create the parallax scrolling effect */
                background-attachment: fixed;
                background-position: center;
                background-repeat: no-repeat;
                -webkit-background-size: cover;
                -moz-background-size: cover;
                -o-background-size: cover;
                background-size: cover;
            }
            a{
                color: #f2f2f2;
                text-decoration: none;
                list-style-type: none;
            }
            nav {
                padding: 0px;
                text-align: center;
                position: fixed;
                margin:-8px;
                background-color:#333;
                width: 100%;
                font-family: 'PT Sans', sans-serif;
                line-height:65px;
    
            }
            nav > ul {
                list-style: none;
                padding: 0;
                margin: 0;
                display: inline-block;
                background: #333;
    
                float: left;
            }
            nav > ul > li {
                float: left;
                width: 140px;
                height: 65px;
                font-size: large;
                position: relative;
                text-transform: uppercase;
                font-size: 17px;
                color: #f2f2f2;
                cursor: pointer;
                transition: 0.2s;
            }
            nav > ul > li:hover {
                background: #888;
    
            }
    
            ul.drop-menu {
                position: absolute;
                top: 100%;
                left: 0%;
                width: 100%;
                padding: 0;
            }
            ul.drop-menu li {
                background: #ababab;
                color: rgba(255, 255, 255, 0.7);
            }
            ul.drop-menu li:hover {
                background: #606060;
            }
            ul.drop-menu li:last-child {
                border-radius: 0px 0px 5px 5px;
            }
    
            ul.drop-menu li {
                display: none;
            }
    
            li:hover > ul.drop-menu li {
                display: block;
            }
            #logo{
                cursor: none;
                font-size: 40px;
                float: left;
                left: 0;
                padding: -5px;
                margin-left: -10px;
                font-family: 'PT Sans', sans-serif;
                color: #808080;
            }
            #logo:hover{
                background: #333;
                cursor: default;
            }
    
            #main-heading{
                margin: 40px;
                margin-bottom: 15px;
                margin-left: 10px;
                padding-bottom: -20px ;
                font-size: 50px;
                border-bottom: solid 2.5px darkcyan;
                font-family: 'Teko', sans-serif;
                color: #333333;
    
            }
            .content{
                font-family: 'Roboto Condensed', sans-serif;
                font-size: 20px;
    
                margin: 20px; margin-left: 15px;
            }
    
            .sub-heading{
                margin: 20px;
                margin-left: 15px;
                font-family: 'Roboto Condensed',sans-serif;
                font-size: 30px;
                color: #ff5f2a;
            }
            .special{
                color:#efefef; background-color: #444; display: inline; border: solid #38a8d1 2px;padding: 3px;
            }

            .floatRight{
                float: right;
            }

            .link-color{
                color: #0000FF;
            }

        </style>
    </head>
    <body>
        <nav id="Navigation Container">
            <section>
                <ul>
                    <a id="logo">
                        CS 269 Group Project - Team 10
                    </a>
                </ul> 
            </section>
            <ul class="floatRight"> 
                <li>
                    <a href="#team">Team</a>   
                </li>
                <li>
                    <a href="#abstract">Abstract</a>   
                </li>
                <li>
                    <a href="#images">Images</a>   
                </li>
                <li>
                    <a href="#paper">Paper</a>   
                </li>
                <li>
                    <a href="#result">Results</a>   
                </li>
                <li>
                    <a href="#video">Video</a>   
                </li>
                <li>
                    <a href="#appendix">Appendix</a>   
                </li>
            </ul>
        </nav>
        <div class="parallax"></div>
    
        <div id="team" class="subjects">
            <div id="main-heading">TEAM 10 DETAILS</div>
            <div class="content">
                <ul>
                    <li>Abirami Anbumani
                        (005526158)
                    </li>
                    <li>Aishwarya Dev
                        (305515097)
                    </li>
                    <li>Harini Suresh
                        (505712718)
                    </li>
                    <li>Harshita Khandelwal
                        (905526234)
                    </li>
                    <li>Shardul Parab
                        (205525006)
                    </li>             
                </ul>
            </div>
        </div>

        <div id="abstract" class="subjects">
            <div id="main-heading">ABSTRACT</div>
            <div class="content">
                <p>
                    Object detection is extensively performed by Convolutional Neural networks(CNN)
                    and their variations. MobileNet is one lightweight deep learning-based single
                    shot detector with embedded vision applications. CNNs are inherently limited
                    to model geometric transformations due to the fixed geometric structures in their
                    building modules. Deformable models strengthen this transformation capacity
                    by embedding object information into the system. In this paper, we describe
                    our work titled ’PanduNet,’ a Deformable Convolutions incorporated MobileNet
                    with application in Pedestrian Detection. We have compared and summarized the
                    results of traditional MobileNet and PanduNet for the same. Furthermore, we have
                    summarized our work on pedestrian tracking using PanduNet.
                </p>
            </div>
        </div>

        <div id="images" class="subjects">
            <div id="main-heading">IMAGES</div>
            <div class="content">
                
            </div>
        </div>

        <div id="paper" class="subjects">
            <div id="main-heading">PAPER</div>
            <div class="content">
                Link to the can be found 
                <a class="link-color" href="https://www.overleaf.com/project/61b91651bd513eb374c5272c">here</a>
            </div>
        </div>

        <div id="result" class="subjects">
            <div id="main-heading">RESULTS</div>
            <div class="content">
                <p>
                    The experiments are based on a comparative study between a mobilenet SSD model (without any
                    deformable convolutional units) and the PanduNet model which promises to produce much better
                    results if allowed to train for the entire run i.e. 200 epochs. As explained previously, we do add
                    and alter the base Mobilnet SSD model by adding deformable convolution units in all the extra
                    layers after the mobilenet model. Currently, we set out to train both the models using the transfer
                    learning approach by freezing the Mobilnet[1] Architecture trained on Imagenet dataset[2]. In ideal
                    conditions, the expected training schedule should include using a batch size of 32 for 200 epochs
                    using a cosine scheduler with an initial learning rate of 0.01. However, due to the current lack of
                    resources we have to have done our comparative study on a highly pessimistic training plan, training
                    for 23 epochs on the Pascal VOC dataset for both the base Mobilenet SSD and Pandunet using 1500
                    data points for training and 300 images for testing with a batch size of 4(cannot exceed the batch size
                    4 due to memory constraints). The training is completed on a 4GB 1050Ti NVIDIA GPU and the
                    codebase is majorly written using the Pytorch framework in Python.
                    <br>
                    As described in the Mobilenet paper, we will use the essential data augmentation steps in order to
                    avoid any gaps in the training process. The paper states the following steps:
                    – Use the entire original input image.
                    – Sample a patch so that the minimum jaccard overlap with the objects is 0.1, 0.3, 0.5, 0.7, or 0.9.
                    – Randomly sample a patch.
                    <br>
                    We will do a comparative study on the following factors:
                    <ul>
                        <li>Accuracy comparison using MAP on the PASCAL VOC dataset.</li>
                        <li>Accuracy comparison with pedestrians at different size scales i.e. small, medium and large.</li>
                        <li>Visual Study for objects with different orientations.</li>
                        <li>Visual study of handling occlusions.</li>
                    </ul>
                </p>
                <br>
                <ol>
                    <li>
                        <strong>
                            Mean Average Precision
                        </strong>
                    </li>
                    <br>
                    Precision and recall are single-value measurements based on the system’s entire list of documents
                    returned. It is desirable to consider the order in which the returned documents are presented when
                    using systems that return a rated sequence of documents. One can create a precision-recall curve by
                    computing precision and recall at each location in the ranking sequence of documents and graphing
                    precision p(r) as a function of recall r. The average value of p(r) for the range of r = 0 to r = 1 is
                    computed using average precision:
                    <br>
                    <br>
                    <math>
                        A<sub>ve</sub>P =  ∫<sub>0</sub><sup>1</sup> p(r) dr
                    </math>
                    <br>
                    <br>
                    <br>
                    Mean average precision (MAP) for a set of queries is the mean of the average precision scores for
                    each query.
                    <br>
                    <br>
                    <math>
                        MAP = ( ∑<sub>q=1</sub>Q A<sub>ve</sub>P(q) )/Q
                    </math>
                    <br>
                    <br>
                    <br>
                    <li>
                        <strong>
                            Results on PASCAL VOC Dataset
                        </strong>                        
                    </li>
                    <br>
                    As stated previously, we are using a subset of the Pascal VOC dataset using 1500 data points and 300 images. The metric used for the following is the Mean Average Precision. In the current case after 23 epochs the MAP score of the base mobilenet SSD model is 32.39 whereas for our Pandunet we get an MAP of 43.59 at only 23 epochs which is a huge improvement. This also shows that our model has a tendency to converge much earlier and in a much more accurate manner. Lastly, we predict that if we had more resources and time, the model would have definitely shown a vast improvement over the previously existing MobileNet SSD.
                    <br>
                    <br>
                    <br>
                    <li>
                        <strong>
                            Results on Accuracy comparison with pedestrians at different size scales i.e. small, medium and large.                       
                        </strong> 
                    </li>
                    <br>
                    The MAP score is an indication of the entire dataset but does not give an idea about the minute details. So we have also conducted a study on pedestrians of different sizes i.e. small, medium, large, etc. It could be a case that the model could become biased to only images of a particular size and it can definitely help in dissecting the model for further improvements. For this case we use the COCO dataset and compare small, medium and large. These Average Precision Scales are strictly predefined as objects smaller than 32x32 to greater than 96x96 pixels. The results of the same are as follows: small - mobilenetSSD is 25.32 and Pandunet is 33.42. For medium scaled images - mobilenetSSD is 28.66 and Pandunet is 33.96. However, for large scaled humans in the dataset, mobilenetSSD scores 27.33 and Pandunet scores 27.12. So in the majority of cases, Pandunet outscores mobilenetSSD and can be generalized for pedestrian detection. Additionally, it is expected that for pedestrians the small and medium sized humans would be the case in general and large sized cases would be rare. Although in the future, it needs to be seen as to why large objects are not giving optimal performance with Pandunet.
                    <br>
                    <br>
                    <br>
                    <li>
                        <strong>
                            Visual Study for handling objects with different orientations:
                        </strong> 
                    </li>
                    <br>
                    As per the paper on Deformable Convolutions, it is expected that the detector using Deform Convolutions should work with objects at different orientations and give consistent results for the same. To test this hypothesis, we run our model on images with different orientations and test how well the model performs. Based on testing a  few detectors such as Yolov1, mobilenet-SSD It is evident that object detectors are in general not actually trained for this scenario. However, upon evaluating the performance of PanduNet on a very small dataset prepared by the team by rotating images, we have observed a positive performance. This does present optimism that the model can give really good results upon complete training.
                    <br>
                    <br>
                    <br>
                    <li>
                        <strong>
                            Visual study of handling occlusions:
                        </strong> 
                    </li>
                    <br>
                    Detection of occluded pedestrians is a big issue that needs to be solved for consistent detection as well as tracking. The salient features of deformable convolutions does provide a reason to believe that PanduNet can handle occlusions too to a considerable extent. We present a few useful cases where PaduNet has successfully managed to provide positive results of occluded/hidden pedestrians. This is also a positive outcome wherein upon complete training, Pandunet can also help in providing consistent trackers and help develop consistent and robust detectors in the long term.
                    <br>
                </ol>
            </div>
        </div>

        <div id="video" class="subjects">
            <div id="main-heading">VIDEO</div>
            <div class="content">
                
            </div>
        </div>

        <div id="appendix" class="subjects">
            <div id="main-heading">APPENDIX</div>
            <div class="content">
                
            </div>
        </div>
        <br>
        <br>
        </div>
    </body>
</html>
